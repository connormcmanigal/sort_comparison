{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DATA260P Project 1: Comparing Sorting Algorithms**\n",
    "\n",
    "##### Connor McManigal and Peyton Politewicz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tr_df = pd.read_csv('tr_table.csv')\n",
    "as_df = pd.read_csv('as_table.csv')\n",
    "\n",
    "def get_theoretical_big_o(algo):\n",
    "    if algo in ['Merge', 'Simple Tim']:\n",
    "        return 'n log n'\n",
    "    elif algo in ['Quick', 'Insertion', 'Shell731', 'Shell1000', 'Bucket', 'Binary Insertion']:\n",
    "        return 'n^2'\n",
    "    elif algo == 'Radix':\n",
    "        return 'nd'\n",
    "    else:\n",
    "        return 'Unknown'  # Just in case I mess up\n",
    "\n",
    "tr_df['Theoretical Big-O'] = tr_df['Algo'].apply(get_theoretical_big_o)\n",
    "as_df['Theoretical Big-O'] = as_df['Algo'].apply(get_theoretical_big_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Algo  Data Size  Observed Runtime     Ratio  Emp Big-O  \\\n",
      "0              Merge       1000          0.001358       NaN        NaN   \n",
      "1              Merge       2000          0.002968  2.185281   1.127819   \n",
      "2              Merge       4000          0.006353  2.140799   1.098149   \n",
      "3              Merge       8000          0.013436  2.114739   1.080480   \n",
      "4              Merge      16000          0.028682  2.134772   1.094082   \n",
      "5              Quick       1000          0.000983       NaN        NaN   \n",
      "6              Quick       2000          0.002183  2.220533   1.150906   \n",
      "7              Quick       4000          0.004996  2.288156   1.194186   \n",
      "8              Quick       8000          0.012189  2.439670   1.286686   \n",
      "9              Quick      16000          0.031078  2.549712   1.350334   \n",
      "10         Insertion       1000          0.014235       NaN        NaN   \n",
      "11         Insertion       2000          0.059809  4.201434   2.070882   \n",
      "12         Insertion       4000          0.237579  3.972304   1.989976   \n",
      "13         Insertion       8000          0.959732  4.039635   2.014225   \n",
      "14         Insertion      16000          3.863937  4.026060   2.009369   \n",
      "15          Shell731       1000          0.004920       NaN        NaN   \n",
      "16          Shell731       2000          0.018754  3.811846   1.930490   \n",
      "17          Shell731       4000          0.072155  3.847520   1.943929   \n",
      "18          Shell731       8000          0.280652  3.889582   1.959615   \n",
      "19          Shell731      16000          1.113573  3.967800   1.988339   \n",
      "20         Shell1000       1000          0.003393       NaN        NaN   \n",
      "21         Shell1000       2000          0.010192  3.003536   1.586662   \n",
      "22         Shell1000       4000          0.028034  2.750647   1.459771   \n",
      "23         Shell1000       8000          0.078536  2.801432   1.486165   \n",
      "24         Shell1000      16000          0.220780  2.811172   1.491172   \n",
      "25            Bucket       1000          0.000166       NaN        NaN   \n",
      "26            Bucket       2000          0.000308  1.854998   0.891417   \n",
      "27            Bucket       4000          0.001994  6.475608   2.695016   \n",
      "28            Bucket       8000          0.000865  0.434106  -1.203880   \n",
      "29            Bucket      16000          0.001611  1.861552   0.896506   \n",
      "30             Radix       1000          0.000544       NaN        NaN   \n",
      "31             Radix       2000          0.001131  2.079548   1.056270   \n",
      "32             Radix       4000          0.002226  1.968889   0.977382   \n",
      "33             Radix       8000          0.004393  1.973605   0.980833   \n",
      "34             Radix      16000          0.008782  1.998955   0.999246   \n",
      "35  Binary Insertion       1000          0.001888       NaN        NaN   \n",
      "36  Binary Insertion       2000          0.005836  3.091340   1.628233   \n",
      "37  Binary Insertion       4000          0.021105  3.616503   1.854595   \n",
      "38  Binary Insertion       8000          0.090807  4.302527   2.105184   \n",
      "39  Binary Insertion      16000          0.382527  4.212543   2.074692   \n",
      "40        Simple Tim       1000          0.001106       NaN        NaN   \n",
      "41        Simple Tim       2000          0.002461  2.225071   1.153852   \n",
      "42        Simple Tim       4000          0.005390  2.190182   1.131051   \n",
      "43        Simple Tim       8000          0.011606  2.153265   1.106526   \n",
      "44        Simple Tim      16000          0.025035  2.156951   1.108993   \n",
      "\n",
      "   Theoretical Big-O  \n",
      "0            n log n  \n",
      "1            n log n  \n",
      "2            n log n  \n",
      "3            n log n  \n",
      "4            n log n  \n",
      "5                n^2  \n",
      "6                n^2  \n",
      "7                n^2  \n",
      "8                n^2  \n",
      "9                n^2  \n",
      "10               n^2  \n",
      "11               n^2  \n",
      "12               n^2  \n",
      "13               n^2  \n",
      "14               n^2  \n",
      "15               n^2  \n",
      "16               n^2  \n",
      "17               n^2  \n",
      "18               n^2  \n",
      "19               n^2  \n",
      "20               n^2  \n",
      "21               n^2  \n",
      "22               n^2  \n",
      "23               n^2  \n",
      "24               n^2  \n",
      "25               n^2  \n",
      "26               n^2  \n",
      "27               n^2  \n",
      "28               n^2  \n",
      "29               n^2  \n",
      "30                nd  \n",
      "31                nd  \n",
      "32                nd  \n",
      "33                nd  \n",
      "34                nd  \n",
      "35               n^2  \n",
      "36               n^2  \n",
      "37               n^2  \n",
      "38               n^2  \n",
      "39               n^2  \n",
      "40           n log n  \n",
      "41           n log n  \n",
      "42           n log n  \n",
      "43           n log n  \n",
      "44           n log n  \n"
     ]
    }
   ],
   "source": [
    "print(tr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Experimental Time Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MergeSort Time Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Merge1](./figure/mergetr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Merge2](./figure/mergeas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Merge3](./figure/mergerun.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QuickSort Time Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Quick1](./figure/quicktr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Quick2](./figure/quickas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Quick3](./figure/quickrun.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InsertionSort Time Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Insertion1](./figure/insertiontr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Insertion2](./figure/insertionas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Insertion3](./figure/insertionrun.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ShellSort Time Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Shell1](./figure/shelltr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Shell2](./figure/shellas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Shell3](./figure/shell1run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Shell4](./figure/shell2run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BucketSort Time Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bucket1](./figure/buckettr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bucket2](./figure/bucketas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bucket3](./figure/bucketrun.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RadixSort Time Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Radix1](./figure/radixtr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Radix2](./figure/radixas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Radix3](./figure/radixrun.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BinaryInsertionSort Time Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote the BinaryInsertionSort algorithm in an effort to improve runtime from the slow and clunky InsertionSort implementation(it appeared to be the slowest of our algorithms). After running InsertionSort and observing ~4 second runtimes on the larger data size(16000), I wanted to find an approach that could drastically enhance its performance on large dataset sizes. I used two helper functions, one to perform the binary search to find the correct position to insert an element into the sorted subarray(binary_search()) and the other to execute the sorting logic in conjunction with the binary search mechanism(sort()). After completing my implementation for BinaryInsertionSort, both the truly random and almost sorted data of size 16000 saw immense improvements: roughly ~4 seconds runtimes on truly random and almost sorted data of size 16000 with InsertionSort to under 0.4 seconds with BinaryInsertionSort. BinaryInsertionSort roughly improved runtime from InsertionSort by around 90%. (Connor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BinaryInsertionSort Natural Language PseudoCode**:\n",
    "\n",
    "*Input*: truly random generated array or almost sorted array of numbers\n",
    "*Output*: array in ascending order\n",
    "\n",
    "        1. (sort()) For each element (starting from the second element) in the array:\n",
    "                1.a Set \"current\" to the element at the current index of the loop\n",
    "                1.b Set \"j\" to a binary_search() call to find the correct position to insert \"current\" into the sorted subarray\n",
    "                        1.bi (nested binary_search()) While the start index \"start\" is less than the end index \"end\":\n",
    "                                1.bi(a) Calculate the midpoint index \"mid\" by finding the halfway point of \"start\" and \"end\"\n",
    "                                1.bi(b) If the value of the midpoint \"mid\" is less than the target value \"value\":\n",
    "                                        1.bi(bi) Set the start index \"start\" to the midpoint plus 1 \"mid + 1\"\n",
    "                                1.bi(c) Else:\n",
    "                                        1.bi(ci) Set the end index \"end\" to the midpoint index \"mid\"\n",
    "                        1.bii Return the start index \"start\" as the position for which the \"value\" should be inserted\n",
    "                1.c Shift elements from \"data\" index \"i - 1\" to \"j + 1\" by one position to make room for the \"current\" element\n",
    "                1.d Place the \"current\" element at index \"j\" of \"data\"\n",
    "        2. Return the sorted array \"data\"\n",
    "\n",
    "- *Input for binary_search()*: sorted array \"data\", value to be searched for \"value\"(\"current\" in sort()), start index of array \"start\", and end idex of array \"end\"\n",
    "- *Output for binary_search()*: index where target value should be inserted\n",
    "\n",
    "**BinaryInsertionSort PsuedoCode**:\n",
    "\n",
    "    class BinaryInsertionSort(CustomSort1):\n",
    "        def __init__(self,):\n",
    "            self.time = 0\n",
    "\n",
    "        def binary_search(self, data to be sorted, target value for insertion, start index, end index):\n",
    "            while start index < end index:\n",
    "                midpoint index = (start index + end index) // 2\n",
    "                if data to be sorted[midpoint index] < target value:\n",
    "                    start index = midpoint + 1\n",
    "                else:\n",
    "                    end index = midpoint index\n",
    "            return start index\n",
    "    \n",
    "        def sort(self, data to be sorted):\n",
    "            for index i from 1 to length(data) - 1:\n",
    "                current value = data to be sorted[ index i]\n",
    "                index j = binary_search(data to be sorted, current value, 0, index i)\n",
    "                data to be sorted[index j + 1: index i + 1] = data to be sorted[index j:index i]\n",
    "                data to be sorted[index j] = current value\n",
    "            return data sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the runtime improvements from InsertionSort to BinaryInsertionSort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of BinaryInsertionSort to InsertionSort runtime on True Random data:\n",
      "           BIS Runtime  Insertion Runtime  Runtime Ratio (BIS / Insertion)\n",
      "Data Size                                                                 \n",
      "1000          0.001888           0.014235                         0.132614\n",
      "2000          0.005836           0.059809                         0.097575\n",
      "4000          0.021105           0.237579                         0.088835\n",
      "8000          0.090807           0.959732                         0.094617\n",
      "16000         0.382527           3.863937                         0.098999\n"
     ]
    }
   ],
   "source": [
    "bis_df = tr_df.loc[tr_df['Algo'] == 'Binary Insertion', ['Data Size', 'Observed Runtime']].copy()\n",
    "bis_df.rename(columns={'Observed Runtime': 'BIS Runtime'}, inplace=True)\n",
    "\n",
    "insertion_df = tr_df.loc[tr_df['Algo'] == 'Insertion', ['Data Size', 'Observed Runtime']].copy()\n",
    "insertion_df.rename(columns={'Observed Runtime': 'Insertion Runtime'}, inplace=True)\n",
    "\n",
    "comparison_df = pd.merge(bis_df, insertion_df, on='Data Size')\n",
    "comparison_df['Runtime Ratio (BIS / Insertion)'] = comparison_df['BIS Runtime'] / comparison_df['Insertion Runtime']\n",
    "comparison_df.set_index('Data Size', inplace=True)\n",
    "\n",
    "print(\"Comparison of BinaryInsertionSort to InsertionSort runtime on True Random data:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of BinaryInsertionSort to InsertionSort runtime on True Random data:\n",
      "           BIS Runtime  Insertion Runtime  Runtime Ratio (BIS / Insertion)\n",
      "Data Size                                                                 \n",
      "1000          0.001852           0.014492                         0.127823\n",
      "2000          0.005927           0.062298                         0.095140\n",
      "4000          0.021826           0.247955                         0.088024\n",
      "8000          0.091293           1.004275                         0.090904\n",
      "16000         0.383837           4.016604                         0.095562\n"
     ]
    }
   ],
   "source": [
    "bis_df = as_df.loc[as_df['Algo'] == 'Binary Insertion', ['Data Size', 'Observed Runtime']].copy()\n",
    "bis_df.rename(columns={'Observed Runtime': 'BIS Runtime'}, inplace=True)\n",
    "\n",
    "insertion_df = as_df.loc[as_df['Algo'] == 'Insertion', ['Data Size', 'Observed Runtime']].copy()\n",
    "insertion_df.rename(columns={'Observed Runtime': 'Insertion Runtime'}, inplace=True)\n",
    "\n",
    "comparison_df = pd.merge(bis_df, insertion_df, on='Data Size')\n",
    "comparison_df['Runtime Ratio (BIS / Insertion)'] = comparison_df['BIS Runtime'] / comparison_df['Insertion Runtime']\n",
    "comparison_df.set_index('Data Size', inplace=True)\n",
    "\n",
    "print(\"Comparison of BinaryInsertionSort to InsertionSort runtime on True Random data:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, these results clearly illustrate the substantial runtime improvements achieved by BinaryInsertionSort. Across both true random and almost sorted inputs, BinaryInsertionSort consistently demonstrated lower mean runtimes compared to InsertionSort. The above two tables show that as the size of the input data increases, the runtime ratio of BinaryInsertionSort to InsertionSort remains relatively stable, ranging from 0.09 to 0.13. These ratios reflect that BinaryInsertionSort improved run times by 88-92%. This illustrates how the combination of insertion sort and binary search is more efficient in terms of runtime than InsertionSort alone(regardless of the data size). By halving the search space with each comparison, it reduced the total number of comparisons needed to find the insertion index, thus leading to faster runtimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BIS1](./figure/bistr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BIS2](./figure/bisas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BIS3](./figure/bisrun.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Timsort Time Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timsort was an appealing discovery during my research into iterative improvements upon these sorting algorithms, as Timsort's most robust and feature-complete version is actually used at the core of Python's built-in sort() and sorted() functions. I sought to duplicate at least some of its functionality - in particular, its utilization of building 'runs' with insertion sort, that are then brought together with mergesort. This 'run' component is the only aspect of its robustness I sought to integrate for performance gains in our relatively straightforward use case.\n",
    "\n",
    "# Timsort Pseudocode\n",
    "\n",
    "Class Timsort:\n",
    "    Initialize with some minimum length of each 'run':\n",
    "        Set MIN_RUN = 32\n",
    "\n",
    "    'sort' method, taking parameter 'data':\n",
    "        Call recursive timsort_basic method, passing 'data'\n",
    "        Return sorted 'data' upon completion of recursive sort\n",
    "\n",
    "    'timsort_basic' method with parameter 'data':\n",
    "        Set 'n' to the length of 'data'\n",
    "        Create runs of at least MIN_RUN size using 'insertion_sort'\n",
    "        \n",
    "        Initialize 'size' to MIN_RUN\n",
    "        While 'size' is less than 'n' (merge the array, iteratively doubling the size of chunks to be merged):\n",
    "            For each 'left' starting from 0, stepping by '2 * size':\n",
    "                Calculate midpoint 'mid' as minimum of 'n - 1' and 'left + size - 1'\n",
    "                Calculate 'right' as minimum of '(left + 2 * size - 1)' and '(n - 1)'\n",
    "                If 'mid' is less than 'right', merge the current sections\n",
    "            Double the 'size'\n",
    "\n",
    "    'insertion_sort' method with parameters 'data', 'left', 'right':\n",
    "        For each position 'i' in range from 'left + 1' to 'right':\n",
    "            Set 'key' to the value of 'data' at index 'i'\n",
    "            Initialize 'j' to 'i - 1'\n",
    "            While 'j' is greater than or equal to 'left' and 'data[j]' is greater than 'key':\n",
    "                Move 'data[j]' one position to the right\n",
    "                Decrease 'j' by 1\n",
    "            Place 'key' in the correct sorted position\n",
    "\n",
    "    'merge' method with parameters 'data', 'left', 'mid', 'right':\n",
    "        Initialize an empty list 'temp'\n",
    "        Set 'i' to 'left' and 'j' to 'mid + 1'\n",
    "        While either 'i' is less than or equal to 'mid' or 'j' is less than or equal to 'right':\n",
    "            Compare elements from both halves and append the smaller one to 'temp'\n",
    "            Increment 'i' or 'j' accordingly\n",
    "        Append any remaining elements from either half to 'temp'\n",
    "        Copy 'temp' back into 'data' starting from index 'left'\n",
    "\n",
    "Below, let's look at how this simplified timsort improves upon mergesort performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Simple Timsort to MergeSort runtime on True Random data:\n",
      "           Simple Tim Runtime  Merge Runtime  \\\n",
      "Data Size                                      \n",
      "1000                 0.001106       0.001358   \n",
      "2000                 0.002461       0.002968   \n",
      "4000                 0.005390       0.006353   \n",
      "8000                 0.011606       0.013436   \n",
      "16000                0.025035       0.028682   \n",
      "\n",
      "           Runtime Ratio (Simple Tim / Merge)  \n",
      "Data Size                                      \n",
      "1000                                 0.814431  \n",
      "2000                                 0.829260  \n",
      "4000                                 0.848390  \n",
      "8000                                 0.863846  \n",
      "16000                                0.872820  \n"
     ]
    }
   ],
   "source": [
    "simple_tim_df = tr_df.loc[tr_df['Algo'] == 'Simple Tim', ['Data Size', 'Observed Runtime']].copy()\n",
    "simple_tim_df.rename(columns={'Observed Runtime': 'Simple Tim Runtime'}, inplace=True)\n",
    "\n",
    "merge_df = tr_df.loc[tr_df['Algo'] == 'Merge', ['Data Size', 'Observed Runtime']].copy()\n",
    "merge_df.rename(columns={'Observed Runtime': 'Merge Runtime'}, inplace=True)\n",
    "\n",
    "comparison_df = pd.merge(simple_tim_df, merge_df, on='Data Size')\n",
    "\n",
    "comparison_df['Runtime Ratio (Simple Tim / Merge)'] = comparison_df['Simple Tim Runtime'] / comparison_df['Merge Runtime']\n",
    "\n",
    "comparison_df.set_index('Data Size', inplace=True)\n",
    "\n",
    "print(\"Comparison of Simple Timsort to MergeSort runtime on True Random data:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Simple Timsort to MergeSort runtime on Almost-sorted data:\n",
      "           Simple Tim Runtime  Merge Runtime  \\\n",
      "Data Size                                      \n",
      "1000                 0.001088       0.001372   \n",
      "2000                 0.002509       0.003013   \n",
      "4000                 0.005462       0.006403   \n",
      "8000                 0.011643       0.013497   \n",
      "16000                0.025603       0.028850   \n",
      "\n",
      "           Runtime Ratio (Simple Tim / Merge)  \n",
      "Data Size                                      \n",
      "1000                                 0.793026  \n",
      "2000                                 0.832756  \n",
      "4000                                 0.853102  \n",
      "8000                                 0.862636  \n",
      "16000                                0.887475  \n"
     ]
    }
   ],
   "source": [
    "simple_tim_df = as_df.loc[as_df['Algo'] == 'Simple Tim', ['Data Size', 'Observed Runtime']].copy()\n",
    "simple_tim_df.rename(columns={'Observed Runtime': 'Simple Tim Runtime'}, inplace=True)\n",
    "\n",
    "merge_df = as_df.loc[as_df['Algo'] == 'Merge', ['Data Size', 'Observed Runtime']].copy()\n",
    "merge_df.rename(columns={'Observed Runtime': 'Merge Runtime'}, inplace=True)\n",
    "\n",
    "comparison_df = pd.merge(simple_tim_df, merge_df, on='Data Size')\n",
    "\n",
    "comparison_df['Runtime Ratio (Simple Tim / Merge)'] = comparison_df['Simple Tim Runtime'] / comparison_df['Merge Runtime']\n",
    "\n",
    "comparison_df.set_index('Data Size', inplace=True)\n",
    "\n",
    "print(\"Comparison of Simple Timsort to MergeSort runtime on Almost-sorted data:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Timsort Time Analysis\n",
    "\n",
    "We can see that this simple implementation of Timsort provides a modest runtime improvement over MergeSort at the data sizes under consideration. While the performance delta is shrinking as n grows (from approximately a 20% improvement at n = 1000, to a 12% improvement at n = 16,000), this could be potentially be mitigated by adjusting Simple Timsort's starting size of calculated runs, perhaps seeding it as a log-base-two value that scales depending on n. We also see that Timsort is one of the algorithms that suffers a performance hit when working with almost-sorted data, likely derived from the fact that it uses insertion sort as one of its internal mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MOM1](./figure/momtr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MOM2](./figure/momas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MOM3](./figure/momrun.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Comparative Time Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our comparative time analysis, let's bring in some code and import results.\n",
    "\n",
    "# Ranking Table, per data size: True Random permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Random execution time rankings, per data size.\n",
      "                          1000                          2000   \\\n",
      "1            Bucket (0.000166s)            Bucket (0.000308s)   \n",
      "2             Radix (0.000544s)             Radix (0.001131s)   \n",
      "3             Quick (0.000983s)             Quick (0.002183s)   \n",
      "4        Simple Tim (0.001106s)        Simple Tim (0.002461s)   \n",
      "5             Merge (0.001358s)             Merge (0.002968s)   \n",
      "6  Binary Insertion (0.001888s)  Binary Insertion (0.005836s)   \n",
      "7         Shell1000 (0.003393s)         Shell1000 (0.010192s)   \n",
      "8          Shell731 (0.004920s)          Shell731 (0.018754s)   \n",
      "9         Insertion (0.014235s)         Insertion (0.059809s)   \n",
      "\n",
      "                          4000                          8000   \\\n",
      "1            Bucket (0.001994s)            Bucket (0.000865s)   \n",
      "2             Radix (0.002226s)             Radix (0.004393s)   \n",
      "3             Quick (0.004996s)        Simple Tim (0.011606s)   \n",
      "4        Simple Tim (0.005390s)             Quick (0.012189s)   \n",
      "5             Merge (0.006353s)             Merge (0.013436s)   \n",
      "6  Binary Insertion (0.021105s)         Shell1000 (0.078536s)   \n",
      "7         Shell1000 (0.028034s)  Binary Insertion (0.090807s)   \n",
      "8          Shell731 (0.072155s)          Shell731 (0.280652s)   \n",
      "9         Insertion (0.237579s)         Insertion (0.959732s)   \n",
      "\n",
      "                          16000  \n",
      "1            Bucket (0.001611s)  \n",
      "2             Radix (0.008782s)  \n",
      "3        Simple Tim (0.025035s)  \n",
      "4             Merge (0.028682s)  \n",
      "5             Quick (0.031078s)  \n",
      "6         Shell1000 (0.220780s)  \n",
      "7  Binary Insertion (0.382527s)  \n",
      "8          Shell731 (1.113573s)  \n",
      "9         Insertion (3.863937s)  \n"
     ]
    }
   ],
   "source": [
    "data_sizes = tr_df['Data Size'].unique()\n",
    "\n",
    "# Prepare an empty dict to hold the algorithms and their runtimes for each data size\n",
    "rankings_with_runtime = {}\n",
    "\n",
    "for size in data_sizes:\n",
    "    # Filter rows matching current 'Data Size'\n",
    "    filtered_df = tr_df[tr_df['Data Size'] == size]\n",
    "    filtered_df = filtered_df.sort_values(by='Observed Runtime')\n",
    "\n",
    "    # Combine 'Algo' and 'Observed Runtime' into a single string for each row\n",
    "    combined_info = filtered_df.apply(lambda x: \"{} ({:.6f}s)\".format(x['Algo'], x['Observed Runtime']), axis=1).values\n",
    "    \n",
    "    sorted_by_runtime = filtered_df.sort_values(by='Observed Runtime')['Observed Runtime'].values\n",
    "    sorted_combined_info = [info for _,info in sorted(zip(sorted_by_runtime, combined_info))]\n",
    "    \n",
    "    rankings_with_runtime[size] = sorted_combined_info\n",
    "\n",
    "max_length = max(len(v) for v in rankings_with_runtime.values())\n",
    "\n",
    "for size in rankings_with_runtime:\n",
    "    rankings_with_runtime[size] = list(rankings_with_runtime[size]) + [None] * (max_length - len(rankings_with_runtime[size]))\n",
    "\n",
    "tr_ranked_with_runtime_df = pd.DataFrame(rankings_with_runtime)\n",
    "\n",
    "tr_ranked_with_runtime_df.index += 1  # Ranking starts from 1\n",
    "\n",
    "print(\"True Random execution time rankings, per data size.\")\n",
    "print(tr_ranked_with_runtime_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking Table, per data size: Almost-sorted permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Almost-sorted execution time rankings, per data size.\n",
      "                          1000                          2000   \\\n",
      "1            Bucket (0.000180s)            Bucket (0.000323s)   \n",
      "2             Radix (0.000535s)             Radix (0.001149s)   \n",
      "3             Quick (0.000985s)             Quick (0.002114s)   \n",
      "4        Simple Tim (0.001088s)        Simple Tim (0.002509s)   \n",
      "5             Merge (0.001372s)             Merge (0.003013s)   \n",
      "6  Binary Insertion (0.001852s)  Binary Insertion (0.005927s)   \n",
      "7         Shell1000 (0.003496s)         Shell1000 (0.010301s)   \n",
      "8          Shell731 (0.004961s)          Shell731 (0.019170s)   \n",
      "9         Insertion (0.014492s)         Insertion (0.062298s)   \n",
      "\n",
      "                          4000                          8000   \\\n",
      "1            Bucket (0.000517s)            Bucket (0.000871s)   \n",
      "2             Radix (0.002219s)             Radix (0.004375s)   \n",
      "3             Quick (0.004996s)        Simple Tim (0.011643s)   \n",
      "4        Simple Tim (0.005462s)             Quick (0.012218s)   \n",
      "5             Merge (0.006403s)             Merge (0.013497s)   \n",
      "6  Binary Insertion (0.021826s)         Shell1000 (0.079246s)   \n",
      "7         Shell1000 (0.028260s)  Binary Insertion (0.091293s)   \n",
      "8          Shell731 (0.073197s)          Shell731 (0.286670s)   \n",
      "9         Insertion (0.247955s)         Insertion (1.004275s)   \n",
      "\n",
      "                          16000  \n",
      "1            Bucket (0.001638s)  \n",
      "2             Radix (0.008765s)  \n",
      "3        Simple Tim (0.025603s)  \n",
      "4             Merge (0.028850s)  \n",
      "5             Quick (0.030703s)  \n",
      "6         Shell1000 (0.219721s)  \n",
      "7  Binary Insertion (0.383837s)  \n",
      "8          Shell731 (1.125490s)  \n",
      "9         Insertion (4.016604s)  \n"
     ]
    }
   ],
   "source": [
    "data_sizes = as_df['Data Size'].unique()\n",
    "\n",
    "# Prepare an empty dict to hold the algorithms and their runtimes for each data size\n",
    "rankings_with_runtime = {}\n",
    "\n",
    "for size in data_sizes:\n",
    "    # Filter rows matching current 'Data Size'\n",
    "    filtered_df = as_df[as_df['Data Size'] == size]\n",
    "    filtered_df = filtered_df.sort_values(by='Observed Runtime')\n",
    "\n",
    "    # Combine 'Algo' and 'Observed Runtime' into a single string for each row\n",
    "    combined_info = filtered_df.apply(lambda x: \"{} ({:.6f}s)\".format(x['Algo'], x['Observed Runtime']), axis=1).values\n",
    "    \n",
    "    sorted_by_runtime = filtered_df.sort_values(by='Observed Runtime')['Observed Runtime'].values\n",
    "    sorted_combined_info = [info for _,info in sorted(zip(sorted_by_runtime, combined_info))]\n",
    "    \n",
    "    rankings_with_runtime[size] = sorted_combined_info\n",
    "\n",
    "max_length = max(len(v) for v in rankings_with_runtime.values())\n",
    "\n",
    "for size in rankings_with_runtime:\n",
    "    rankings_with_runtime[size] = list(rankings_with_runtime[size]) + [None] * (max_length - len(rankings_with_runtime[size]))\n",
    "\n",
    "as_ranked_with_runtime_df = pd.DataFrame(rankings_with_runtime)\n",
    "\n",
    "as_ranked_with_runtime_df.index += 1  # Ranking starts from 1\n",
    "print(\"Almost-sorted execution time rankings, per data size.\")\n",
    "print(as_ranked_with_runtime_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Observations regarding rankings, patterns, performance as n changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True Random permutation comparison tables between algorithms: Observed runtime, Empirical Big-O, Theoretical Big-O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Random runtimes at Data Size 1000:\n",
      "                Algo  Observed Runtime  Emp Big-O Theoretical Big-O\n",
      "0              Merge          0.001358        NaN           n log n\n",
      "5              Quick          0.000983        NaN               n^2\n",
      "10         Insertion          0.014235        NaN               n^2\n",
      "15          Shell731          0.004920        NaN               n^2\n",
      "20         Shell1000          0.003393        NaN               n^2\n",
      "25            Bucket          0.000166        NaN               n^2\n",
      "30             Radix          0.000544        NaN                nd\n",
      "35  Binary Insertion          0.001888        NaN               n^2\n",
      "40        Simple Tim          0.001106        NaN           n log n\n",
      "True Random runtimes at Data Size 2000:\n",
      "                Algo  Observed Runtime  Emp Big-O Theoretical Big-O\n",
      "1              Merge          0.002968   1.127819           n log n\n",
      "6              Quick          0.002183   1.150906               n^2\n",
      "11         Insertion          0.059809   2.070882               n^2\n",
      "16          Shell731          0.018754   1.930490               n^2\n",
      "21         Shell1000          0.010192   1.586662               n^2\n",
      "26            Bucket          0.000308   0.891417               n^2\n",
      "31             Radix          0.001131   1.056270                nd\n",
      "36  Binary Insertion          0.005836   1.628233               n^2\n",
      "41        Simple Tim          0.002461   1.153852           n log n\n",
      "True Random runtimes at Data Size 4000:\n",
      "                Algo  Observed Runtime  Emp Big-O Theoretical Big-O\n",
      "2              Merge          0.006353   1.098149           n log n\n",
      "7              Quick          0.004996   1.194186               n^2\n",
      "12         Insertion          0.237579   1.989976               n^2\n",
      "17          Shell731          0.072155   1.943929               n^2\n",
      "22         Shell1000          0.028034   1.459771               n^2\n",
      "27            Bucket          0.001994   2.695016               n^2\n",
      "32             Radix          0.002226   0.977382                nd\n",
      "37  Binary Insertion          0.021105   1.854595               n^2\n",
      "42        Simple Tim          0.005390   1.131051           n log n\n",
      "True Random runtimes at Data Size 8000:\n",
      "                Algo  Observed Runtime  Emp Big-O Theoretical Big-O\n",
      "3              Merge          0.013436   1.080480           n log n\n",
      "8              Quick          0.012189   1.286686               n^2\n",
      "13         Insertion          0.959732   2.014225               n^2\n",
      "18          Shell731          0.280652   1.959615               n^2\n",
      "23         Shell1000          0.078536   1.486165               n^2\n",
      "28            Bucket          0.000865  -1.203880               n^2\n",
      "33             Radix          0.004393   0.980833                nd\n",
      "38  Binary Insertion          0.090807   2.105184               n^2\n",
      "43        Simple Tim          0.011606   1.106526           n log n\n",
      "True Random runtimes at Data Size 16000:\n",
      "                Algo  Observed Runtime  Emp Big-O Theoretical Big-O\n",
      "4              Merge          0.028682   1.094082           n log n\n",
      "9              Quick          0.031078   1.350334               n^2\n",
      "14         Insertion          3.863937   2.009369               n^2\n",
      "19          Shell731          1.113573   1.988339               n^2\n",
      "24         Shell1000          0.220780   1.491172               n^2\n",
      "29            Bucket          0.001611   0.896506               n^2\n",
      "34             Radix          0.008782   0.999246                nd\n",
      "39  Binary Insertion          0.382527   2.074692               n^2\n",
      "44        Simple Tim          0.025035   1.108993           n log n\n"
     ]
    }
   ],
   "source": [
    "# Get unique 'Data Size' values\n",
    "data_sizes = tr_df['Data Size'].unique()\n",
    "\n",
    "# Dictionary to store DataFrames\n",
    "dfs_by_data_size = {}\n",
    "\n",
    "# Select only the required columns\n",
    "columns_needed = ['Algo', 'Observed Runtime', 'Emp Big-O', 'Theoretical Big-O']\n",
    "\n",
    "for size in data_sizes:\n",
    "    # Filter tr_df for the current 'Data Size' and select only the required columns\n",
    "    df_filtered = tr_df[tr_df['Data Size'] == size][columns_needed].copy()\n",
    "    \n",
    "    # Add the filtered DataFrame to the dictionary, using 'Data Size' as the key\n",
    "    dfs_by_data_size[size] = df_filtered\n",
    "\n",
    "for data_sizes in dfs_by_data_size:\n",
    "    print(f\"True Random runtimes at Data Size {data_sizes}:\")\n",
    "    print(dfs_by_data_size[data_sizes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Almost-sorted permutation comparison tables between algorithms: Observed runtime, Empirical Big-O, Theoretical Big-O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Almost-sorted runtimes at Data Size 1000:\n",
      "                Algo  Observed Runtime  Emp Big-O Theoretical Big-O\n",
      "0              Merge          0.001372        NaN           n log n\n",
      "5              Quick          0.000985        NaN               n^2\n",
      "10         Insertion          0.014492        NaN               n^2\n",
      "15          Shell731          0.004961        NaN               n^2\n",
      "20         Shell1000          0.003496        NaN               n^2\n",
      "25            Bucket          0.000180        NaN               n^2\n",
      "30             Radix          0.000535        NaN                nd\n",
      "35  Binary Insertion          0.001852        NaN               n^2\n",
      "40        Simple Tim          0.001088        NaN           n log n\n",
      "Almost-sorted runtimes at Data Size 2000:\n",
      "                Algo  Observed Runtime  Emp Big-O Theoretical Big-O\n",
      "1              Merge          0.003013   1.135242           n log n\n",
      "6              Quick          0.002114   1.101992               n^2\n",
      "11         Insertion          0.062298   2.103928               n^2\n",
      "16          Shell731          0.019170   1.950063               n^2\n",
      "21         Shell1000          0.010301   1.558917               n^2\n",
      "26            Bucket          0.000323   0.844827               n^2\n",
      "31             Radix          0.001149   1.101922                nd\n",
      "36  Binary Insertion          0.005927   1.677908               n^2\n",
      "41        Simple Tim          0.002509   1.205767           n log n\n",
      "Almost-sorted runtimes at Data Size 4000:\n",
      "                Algo  Observed Runtime  Emp Big-O Theoretical Big-O\n",
      "2              Merge          0.006403   1.087564           n log n\n",
      "7              Quick          0.004996   1.240900               n^2\n",
      "12         Insertion          0.247955   1.992818               n^2\n",
      "17          Shell731          0.073197   1.932952               n^2\n",
      "22         Shell1000          0.028260   1.455995               n^2\n",
      "27            Bucket          0.000517   0.680581               n^2\n",
      "32             Radix          0.002219   0.950318                nd\n",
      "37  Binary Insertion          0.021826   1.880668               n^2\n",
      "42        Simple Tim          0.005462   1.122388           n log n\n",
      "Almost-sorted runtimes at Data Size 8000:\n",
      "                Algo  Observed Runtime  Emp Big-O Theoretical Big-O\n",
      "3              Merge          0.013497   1.075869           n log n\n",
      "8              Quick          0.012218   1.290173               n^2\n",
      "13         Insertion          1.004275   2.018005               n^2\n",
      "18          Shell731          0.286670   1.969527               n^2\n",
      "23         Shell1000          0.079246   1.487551               n^2\n",
      "28            Bucket          0.000871   0.752056               n^2\n",
      "33             Radix          0.004375   0.979011                nd\n",
      "38  Binary Insertion          0.091293   2.064447               n^2\n",
      "43        Simple Tim          0.011643   1.091902           n log n\n",
      "Almost-sorted runtimes at Data Size 16000:\n",
      "                Algo  Observed Runtime  Emp Big-O Theoretical Big-O\n",
      "4              Merge          0.028850   1.095911           n log n\n",
      "9              Quick          0.030703   1.329332               n^2\n",
      "14         Insertion          4.016604   1.999821               n^2\n",
      "19          Shell731          1.125490   1.973093               n^2\n",
      "24         Shell1000          0.219721   1.471270               n^2\n",
      "29            Bucket          0.001638   0.911686               n^2\n",
      "34             Radix          0.008765   1.002652                nd\n",
      "39  Binary Insertion          0.383837   2.071922               n^2\n",
      "44        Simple Tim          0.025603   1.136866           n log n\n"
     ]
    }
   ],
   "source": [
    "# Get unique 'Data Size' values\n",
    "data_sizes = as_df['Data Size'].unique()\n",
    "\n",
    "# Dictionary to store DataFrames\n",
    "dfs_by_data_size = {}\n",
    "\n",
    "# Select only the required columns\n",
    "columns_needed = ['Algo', 'Observed Runtime', 'Emp Big-O', 'Theoretical Big-O']\n",
    "\n",
    "for size in data_sizes:\n",
    "    # Filter as_df for the current 'Data Size' and select only the required columns\n",
    "    df_filtered = as_df[as_df['Data Size'] == size][columns_needed].copy()\n",
    "    \n",
    "    # Add the filtered DataFrame to the dictionary, using 'Data Size' as the key\n",
    "    dfs_by_data_size[size] = df_filtered\n",
    "\n",
    "for data_sizes in dfs_by_data_size:\n",
    "    print(f\"Almost-sorted runtimes at Data Size {data_sizes}:\")\n",
    "    print(dfs_by_data_size[data_sizes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Assign a common big-o function to each algorithm based on their performance (empirical big-O). Since we're using the doubling hypothesis, we'll derive that from the factor we see in the empirical big-o column (see https://static.us.edusercontent.com/files/rDCyMU3x4VNJBs7AIl82FBnD )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Answer: \"- Do you observe differences between the observed run-time vs the theoretical big-O run time?\" - Respond per-algorithm, we will probably be able to say 'yes' for almost all, since theoretical big-o is worst case scenario and our observed run time/empirical big-o is not worst case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
